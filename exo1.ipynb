{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "866f9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "\"I don’t like the PhD\",\n",
    "\"I don’t like my current job\",\n",
    "\"I want to get a better job\",\n",
    "\"I want to get married\",\n",
    "\"I love to travel\",\n",
    "\"I want to get a scholarship\",\n",
    "\"Duolingo is stupid\",\n",
    "\"Do you like your university ?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6603cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'i': 0, 'don’t': 1, 'like': 2, 'the': 3, 'phd': 4, 'my': 5, 'current': 6, 'job': 7, 'want': 8, 'to': 9, 'get': 10, 'a': 11, 'better': 12, 'married': 13, 'love': 14, 'travel': 15, 'scholarship': 16, 'duolingo': 17, 'is': 18, 'stupid': 19, 'do': 20, 'you': 21, 'your': 22, 'university': 23, '?': 24}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(sentences, columns=[\"sentences\"])\n",
    "\n",
    "word_to_id = {}\n",
    "\n",
    "def create_vocabulary(sentences):\n",
    "    id= 0\n",
    "    for sentence in sentences:\n",
    "        words = sentence.lower().split()\n",
    "        for word in words:\n",
    "            if word not in word_to_id:\n",
    "                word_to_id[word] = id\n",
    "                id += 1 \n",
    "    return word_to_id\n",
    "\n",
    "vocabulary = create_vocabulary(df['sentences'])\n",
    "print(\"Vocabulary:\", vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d07dccb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Co-occurrence Matrix (Window=1) ---\n",
      "            i  like  job  want  travel  university\n",
      "i           0     0    0     3       0           0\n",
      "like        0     0    0     0       0           0\n",
      "job         0     0    0     0       0           0\n",
      "want        3     0    0     0       0           0\n",
      "travel      0     0    0     0       0           0\n",
      "university  0     0    0     0       0           0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "target_words = ['i', 'like', 'job', 'want', 'travel', 'university']\n",
    "n = len(target_words)\n",
    "\n",
    "# Create a mapping from word -> matrix index (0 to 5)\n",
    "word_to_idx = {word: i for i, word in enumerate(target_words)}\n",
    "\n",
    "# 2. Initialize Matrix (6x6)\n",
    "co_occurrence_matrix = np.zeros((n, n), dtype=int)\n",
    "\n",
    "# 3. Build the Matrix\n",
    "window_size = 1\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.lower().split()\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        # Only process if the current word is in our target list\n",
    "        if token in target_words:\n",
    "            current_idx = word_to_idx[token]\n",
    "            \n",
    "            # Check context window (left and right)\n",
    "            # Range: from i - window to i + window\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(tokens), i + window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if i == j: continue # Skip the word itself\n",
    "                \n",
    "                neighbor = tokens[j]\n",
    "                \n",
    "                # If the neighbor is ALSO in our target list, increment count\n",
    "                if neighbor in target_words:\n",
    "                    neighbor_idx = word_to_idx[neighbor]\n",
    "                    co_occurrence_matrix[current_idx][neighbor_idx] += 1\n",
    "\n",
    "# 4. Convert to DataFrame for readability\n",
    "df_co_oc = pd.DataFrame(co_occurrence_matrix, index=target_words, columns=target_words)\n",
    "\n",
    "print(\"--- Co-occurrence Matrix (Window=1) ---\")\n",
    "print(df_co_oc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3468dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabularyulary: {'i': 0, 'don’t': 1, 'like': 2, 'the': 3, 'phd': 4, 'my': 5, 'current': 6, 'job': 7, 'want': 8, 'to': 9, 'get': 10, 'a': 11, 'better': 12, 'married': 13, 'love': 14, 'travel': 15, 'scholarship': 16, 'duolingo': 17, 'is': 18, 'stupid': 19, 'do': 20, 'you': 21, 'your': 22, 'university': 23, '?': 24}\n",
      "\n",
      "Starting Training...\n",
      "Epoch 0: Loss = 3.0324\n",
      "Epoch 100: Loss = 1.7065\n",
      "Epoch 200: Loss = 1.1251\n",
      "Epoch 300: Loss = 0.7673\n",
      "Epoch 400: Loss = 0.5438\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ==========================================\n",
    "# 1. Data Preparation\n",
    "# ==========================================\n",
    "raw_text = \"i don’t like my current job\"\n",
    "tokens = raw_text.lower().split()\n",
    "\n",
    "\n",
    "# Inverse map (ID -> Word) for printing results\n",
    "id_to_word = {v: k for k, v in vocabulary.items()}\n",
    "\n",
    "print(f\"vocabularyulary: {vocabulary}\")\n",
    "\n",
    "# Step 2: Generate Training Data (Context -> Target)\n",
    "WINDOW_SIZE = 2\n",
    "data = []\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    target_word = tokens[i]\n",
    "    target_id = vocabulary[target_word]\n",
    "    \n",
    "    # Get neighbors\n",
    "    start = max(0, i - WINDOW_SIZE)\n",
    "    end = min(len(tokens), i + WINDOW_SIZE + 1)\n",
    "    \n",
    "    context_words = [tokens[j] for j in range(start, end) if j != i]\n",
    "    context_ids = [vocabulary[w] for w in context_words]\n",
    "    \n",
    "    # PADDING: Fill with 0s so all inputs are length 4 (2*window)\n",
    "    max_len = WINDOW_SIZE * 2\n",
    "    while len(context_ids) < max_len:\n",
    "        context_ids.append(0) # Append <PAD> ID\n",
    "        \n",
    "    data.append((context_ids, target_id))\n",
    "\n",
    "# ==========================================\n",
    "# 2. Define the CBOW Model\n",
    "# ==========================================\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        \n",
    "        # 1. Embedding Layer\n",
    "        # padding_idx=0 tells PyTorch not to learn a vector for the <PAD> token\n",
    "        self.embeddings = nn.Embedding(vocabulary_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # 2. Linear Layer (Hidden -> Output)\n",
    "        self.linear = nn.Linear(embedding_dim, vocabulary_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Input shape: [batch_size, context_window_size] -> e.g., [1, 4]\n",
    "        \n",
    "        # Look up embeddings: [1, 4] -> [1, 4, embedding_dim]\n",
    "        embeds = self.embeddings(inputs)\n",
    "        \n",
    "        # Aggregation: Take the MEAN of the context vectors\n",
    "        # Shape becomes: [1, embedding_dim]\n",
    "        hidden = torch.mean(embeds, dim=1)\n",
    "        \n",
    "        # Prediction: Project back to vocabularyulary size\n",
    "        output = self.linear(hidden)\n",
    "        return output\n",
    "\n",
    "# ==========================================\n",
    "# 3. Training Loop\n",
    "# ==========================================\n",
    "EMBED_DIM = 10\n",
    "LEARNING_RATE = 0.05\n",
    "EPOCHS = 500\n",
    "\n",
    "model = CBOW(len(vocabulary), EMBED_DIM)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Convert data to tensors\n",
    "context_data = torch.tensor([item[0] for item in data]) # Shape: [6, 4]\n",
    "target_data = torch.tensor([item[1] for item in data])  # Shape: [6]\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    log_probs = model(context_data)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_function(log_probs, target_data)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f11726b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Specific Case ---\n",
      "Context: ['i', 'don’t', 'my', 'current']\n",
      "Predicted ID: 2\n",
      "Predicted Word: 'like'\n",
      "\n",
      "--- Testing Specific Case ---\n",
      "Context: ['my', 'current']\n",
      "Predicted ID: 7\n",
      "Predicted Word: 'job'\n"
     ]
    }
   ],
   "source": [
    "def interference(test_words):\n",
    "    # Testing (Inference)\n",
    "    print(\"\\n--- Testing Specific Case ---\")\n",
    "    # Let's test the \"job\" example: Context = \"my current\"\n",
    "    test_ids = [vocabulary[w] for w in test_words]\n",
    "\n",
    "    # Pad to length 4\n",
    "    while len(test_ids) < 4:\n",
    "        test_ids.append(0)\n",
    "\n",
    "    # Convert to tensor and predict\n",
    "    input_tensor = torch.tensor([test_ids]) # Shape [1, 4]\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        predicted_id = torch.argmax(output).item()\n",
    "\n",
    "    print(f\"Context: {test_words}\")\n",
    "    print(f\"Predicted ID: {predicted_id}\")\n",
    "    print(f\"Predicted Word: '{id_to_word[predicted_id]}'\")\n",
    "\n",
    "test_words1 = [\"i\", \"don’t\",\"my\",\"current\"]\n",
    "test_words2 = [\"my\", \"current\"]\n",
    "\n",
    "for test_words in [test_words1, test_words2]:\n",
    "    interference(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee08c43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['i', 'want', 'to', 'get', 'married']\n",
      "\n",
      "--- Skip-gram Generation ---\n",
      "Center: 'want' (8) -> Contexts: [0, 11, 12]\n",
      "Center: 'married' (13) -> Contexts: [11, 12]\n",
      "\n",
      "--- Step 3: Comparison ---\n",
      "Total CBOW Samples:      5\n",
      "Total Skip-gram Samples: 14\n",
      "Ratio: 2.8x more samples in Skip-gram\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Setup Data\n",
    "# IDs matching your Exercise 1 & 4\n",
    "word_to_id = {\n",
    "    \"i\": 0, \"want\": 8, \"to\": 11, \"get\": 12, \"married\": 13\n",
    "}\n",
    "id_to_word = {v: k for k, v in word_to_id.items()}\n",
    "\n",
    "sentence = \"i want to get married\"\n",
    "tokens = sentence.lower().split()\n",
    "token_ids = [word_to_id[t] for t in tokens]\n",
    "window_size = 2\n",
    "\n",
    "# ==========================================\n",
    "# Generate Skip-gram Samples\n",
    "# ==========================================\n",
    "skipgram_pairs = []\n",
    "\n",
    "print(f\"Sentence: {tokens}\\n\")\n",
    "print(\"--- Skip-gram Generation ---\")\n",
    "\n",
    "for i, center_id in enumerate(token_ids):\n",
    "    # Determine window range\n",
    "    start = max(0, i - window_size)\n",
    "    end = min(len(token_ids), i + window_size + 1)\n",
    "    \n",
    "    # Get context words (excluding self)\n",
    "    context_ids = [token_ids[j] for j in range(start, end) if j != i]\n",
    "    \n",
    "    # Create pairs: (Center, Context)\n",
    "    for ctx_id in context_ids:\n",
    "        skipgram_pairs.append((center_id, ctx_id))\n",
    "        \n",
    "    # Print specific examples requested in the prompt\n",
    "    center_word = id_to_word[center_id]\n",
    "    if center_word in [\"want\", \"married\"]:\n",
    "        print(f\"Center: '{center_word}' ({center_id}) -> Contexts: {context_ids}\")\n",
    "\n",
    "# ==========================================\n",
    "# Comparison Logic\n",
    "# ==========================================\n",
    "cbow_count = len(token_ids) # One sample per center word\n",
    "skipgram_count = len(skipgram_pairs)\n",
    "\n",
    "print(\"\\n--- Step 3: Comparison ---\")\n",
    "print(f\"Total CBOW Samples:      {cbow_count}\")\n",
    "print(f\"Total Skip-gram Samples: {skipgram_count}\")\n",
    "print(f\"Ratio: {skipgram_count / cbow_count:.1f}x more samples in Skip-gram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51960859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting Vector: [0.2 0.8]\n",
      "Closest word: married\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "words_vecs = {\n",
    "    \"phd\": np.array([0.9, 0.1]),\n",
    "    \"job\": np.array([0.8, 0.2]),\n",
    "    \"better\": np.array([0.1, 0.9]),\n",
    "    \"married\": np.array([0.2, 0.8])\n",
    "}\n",
    "\n",
    "result = words_vecs[\"phd\"] - words_vecs[\"job\"] + words_vecs[\"better\"]\n",
    "print(\"Resulting Vector:\", result)\n",
    "\n",
    "def closest_word(vector, words_vecs):\n",
    "    best_word = None\n",
    "    best_dist = float(\"inf\")\n",
    "\n",
    "    for word, vec in words_vecs.items():\n",
    "        dist = np.linalg.norm(vector - vec)\n",
    "        if dist < best_dist:\n",
    "            best_dist = dist\n",
    "            best_word = word\n",
    "    return best_word\n",
    "\n",
    "print(\"Closest word:\", closest_word(result, words_vecs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578fad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(word, n=3):\n",
    "    return [word[i:i+n] for i in range(len(word)-n+1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
